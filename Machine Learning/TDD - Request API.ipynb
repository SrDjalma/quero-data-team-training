{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ceba157-53d7-42b0-8246-d6532d94db5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import date, timedelta, datetime,timezone\n",
    "import os\n",
    "from pyspark.sql import Window, SparkSession\n",
    "from functools import reduce\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0b59918-3548-4532-bacd-fc9c15f9d529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://api.stockdata.org/v1/data/intraday/\"\n",
    "headers = {\"Authorization\": \"Bearer TA7vdT3Tr7RcWuYNhUvbDiJQqlI0LTA63nSXnu9f\"}\n",
    "nome_acoes = [\"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\", \"TSLA\", \"META\", \"NFLX\", \"NVDA\", \"BABA\", \"ADBE\"]\n",
    "output_file = \"historico_acoes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa6a0076-3366-41ae-a587-c51167880b04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando FULL LOAD...\n",
      "Erro ao buscar dados para NFLX: 402 - {\"error\":{\"code\":\"usage_limit_reached\",\"message\":\"The usage limit for this account has been reached.\"}}\n",
      "Erro ao buscar dados para NVDA: 402 - {\"error\":{\"code\":\"usage_limit_reached\",\"message\":\"The usage limit for this account has been reached.\"}}\n",
      "Erro ao buscar dados para BABA: 402 - {\"error\":{\"code\":\"usage_limit_reached\",\"message\":\"The usage limit for this account has been reached.\"}}\n",
      "Erro ao buscar dados para ADBE: 402 - {\"error\":{\"code\":\"usage_limit_reached\",\"message\":\"The usage limit for this account has been reached.\"}}\n",
      "FULL LOAD concluído. Dados salvos em 'historico_acoes.csv'.\n",
      "Buscando dados incrementais de 2024-12-04T16:00:00+00:00 até 2024-12-17T16:34:49.496374...\n",
      "Erro ao buscar dados para NFLX: 402 - {\"error\":{\"code\":\"usage_limit_reached\",\"message\":\"The usage limit for this account has been reached.\"}}\n",
      "Erro ao buscar dados para NVDA: 402 - {\"error\":{\"code\":\"usage_limit_reached\",\"message\":\"The usage limit for this account has been reached.\"}}\n",
      "Erro ao buscar dados para BABA: 402 - {\"error\":{\"code\":\"usage_limit_reached\",\"message\":\"The usage limit for this account has been reached.\"}}\n",
      "Erro ao buscar dados para ADBE: 402 - {\"error\":{\"code\":\"usage_limit_reached\",\"message\":\"The usage limit for this account has been reached.\"}}\n",
      "INCREMENTAL LOAD concluído.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def obter_dados_acoes(nome_acoes, data_inicio, data_fim):\n",
    "\n",
    "    dados_historicos = []\n",
    "    for acao in nome_acoes:\n",
    "        params = {\n",
    "            \"symbols\": acao,\n",
    "            \"date_from\": \"2023-09-11\",\n",
    "            \"date_to\": \"2023-09-14\",\n",
    "            # \"max_period_days\": 7\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"data\" in data:\n",
    "                for record in data[\"data\"]:\n",
    "                    dados_historicos.append({\n",
    "                        \"date\": record.get(\"timestamp\"),\n",
    "                        \"ticker\": acao,\n",
    "                        \"open\": record.get(\"open\"),\n",
    "                        \"high\": record.get(\"high\"),\n",
    "                        \"low\": record.get(\"low\"),\n",
    "                        \"close\": record.get(\"close\"),\n",
    "                        \"volume\": record.get(\"volume\")\n",
    "                    })\n",
    "        else:\n",
    "            print(f\"Erro ao buscar dados para {acao}: {response.status_code} - {response.text}\")\n",
    "    return dados_historicos\n",
    "\n",
    "def salvar_dados_csv(dados, arquivo):\n",
    "    df = pd.DataFrame(dados)\n",
    "    if not os.path.exists(arquivo):\n",
    "        df.to_csv(arquivo, index=False)\n",
    "    else:\n",
    "        df_existente = pd.read_csv(arquivo)\n",
    "        df_final = pd.concat([df_existente, df]).drop_duplicates(subset=[\"date\", \"ticker\"]).reset_index(drop=True)\n",
    "        df_final.to_csv(arquivo, index=False)\n",
    "\n",
    "print(\"iniciando\")\n",
    "dados_iniciais = obter_dados_acoes(nome_acoes, start_date, end_date)\n",
    "salvar_dados_csv(dados_iniciais, output_file)\n",
    "print(f\"concluido'{output_file}'.\")\n",
    "\n",
    "def incremental_load(arquivo):\n",
    "\n",
    "    if not os.path.exists(arquivo):\n",
    "        print(\"Nenhum arquivo encontrado\")\n",
    "        return\n",
    "    \n",
    "    df_existente = pd.read_csv(arquivo)\n",
    "    ultima_data = pd.to_datetime(df_existente[\"date\"]).max()\n",
    "    \n",
    "    nova_data_inicio = (ultima_data + timedelta(minutes=1)).isoformat()\n",
    "    nova_data_fim = datetime.now().isoformat()\n",
    "    \n",
    "    print(f\"Buscando dados {nova_data_inicio} até {nova_data_fim}...\")\n",
    "    novos_dados = obter_dados_acoes(nome_acoes, nova_data_inicio, nova_data_fim)\n",
    "    salvar_dados_csv(novos_dados, arquivo)\n",
    "    print(\"concluido.\")\n",
    "\n",
    "incremental_load(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3226fdbf-55c9-47aa-9e83-9269e57854ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(output_file)\n",
    "# print(df.head())\n",
    "# print(df.info())\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ed3e15b-201b-4d66-8a97-52242a98f03f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_spark = spark.createDataFrame(df.head(10000))\n",
    "\n",
    "# display(df_spark)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TDD - Request API",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
